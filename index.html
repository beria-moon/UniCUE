
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="generator" content="Hugo 0.88.1" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
	<link rel="stylesheet" href="css/custom.css">
	<link rel="stylesheet" href="css/normalize.css">
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
	<title>UinCUE</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
</head>
<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">
<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">
<div class="container pt-5 mt-5 shadow-lg p-5 mb-5 bg-white rounded text-center">
  <h2>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2>

  <p>
    <a href="">[Paper]</a>
    <span>&nbsp;</span>
    <a href="">[Dataset]</a>
    <span>&nbsp;</span>
    <a href="">[Code]</a>
  </p>

  <p class="fst-italic mb-0">
    Anonymous authors
  </p>
</div>

<p><b>Abstract:</b>
Cued Speech (CS) enhances lipreading by incorporating hand coding, thereby assisting hearing-impaired individuals in better perceiving spoken language. The CS Video-to-Speech (CSV2S) task aims to convert CS videos directly into intelligible speech. While existing approaches typically adopt a two-stage pipeline‚Äîfirst performing CS Recognition (CSR) followed by text-to-speech synthesis‚Äîthis strategy often suffers from error accumulation and semantic misalignment. To address these issues, we propose <b>UniCUE</b>, the first unified framework for directly generating speech from CS videos. UniCUE seamlessly integrates CSR to extract fine-grained visual-semantic cues that enable more accurate and fluent speech synthesis. Its core components include a pose-aware visual processor for capturing detailed lip-hand visual signals, a semantic alignment pool for precise visual-to-semantic mapping, and a VisioPhonetic adapter to facilitate cross-task representation fusion. Extensive experiments on our newly collected Chinese CS dataset demonstrate that UniCUE significantly outperforms existing methods, establishing a new state-of-the-art in the CSV2S task.


<p><b>UniCUE Framework</b></p>
	<p style="text-align: center;">
		<img src="assets/pipeline.png" height="400" width="700">
</p>


    <h2>üìÅ Dataset</h2>
    <ul>
      <li>11,282 videos</li>
      <li>14 subjects (6 hearing, 8 deaf)</li>
      <li>Rich annotations: phonemes, open pose gestures,speech,video</li>
      <li>Format: MP4 +WAV+TXT</li>
    </ul>

    <h2>üí° Key Features</h2>
    <ul>
      <li>Diverse participants: inclusion of both deaf and hearing groups</li>
      <li>Promotes accessibility and cross-modal language research</li>
      <li>Useful for Cued Speech recognition and generation tasks</li>
    </ul>

    <footer>
      <p>¬© 2025| Designed with ‚ù§Ô∏è for open research</p>
    </footer>

  </div>
</body>
</html>
