
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="generator" content="Hugo 0.88.1" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
	<link rel="stylesheet" href="css/custom.css">
	<link rel="stylesheet" href="css/normalize.css">
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
	<title>UinCUE</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
</head>
<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">
<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">
<div class="container pt-5 mt-5 shadow-lg p-5 mb-5 bg-white rounded text-center">
  <h2>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2>

  <p>
    <a href="">[Paper]</a>
    <span>&nbsp;</span>
    <a href="">[Dataset]</a>
    <span>&nbsp;</span>
    <a href="">[Code]</a>
  </p>

  <p class="fst-italic mb-0">
    Anonymous authors
  </p>
</div>

<p><b>Abstract:</b>
Cued Speech (CS) enhances lipreading by incorporating hand coding, thereby assisting hearing-impaired individuals in better perceiving spoken language. The CS Video-to-Speech (CSV2S) task aims to convert CS videos directly into intelligible speech. While existing approaches typically adopt a two-stage pipeline—first performing CS Recognition (CSR) followed by text-to-speech synthesis—this strategy often suffers from error accumulation and semantic misalignment. To address these issues, we propose <b>UniCUE</b>, the first unified framework for directly generating speech from CS videos. UniCUE seamlessly integrates CSR to extract fine-grained visual-semantic cues that enable more accurate and fluent speech synthesis. Its core components include a pose-aware visual processor for capturing detailed lip-hand visual signals, a semantic alignment pool for precise visual-to-semantic mapping, and a VisioPhonetic adapter to facilitate cross-task representation fusion. Extensive experiments on our newly collected Chinese CS dataset demonstrate that UniCUE significantly outperforms existing methods, establishing a new state-of-the-art in the CSV2S task.

<p><b>UniCUE Framework</b></p>
	<p style="text-align: center;">
		<img src="assets/pipeline.png" height="400" width="700"></p>
	
<h2>Comparison with SOTA methods </h2>
<div class="grid-container">
  <div class="grid-item header">CMML</div>
  <div class="grid-item header">EcoCued</div>
  <div class="grid-item header">CSR (ours)</div>
  <div class="grid-item header">Lip2Speech</div>
  <div class="grid-item header">LipVoicer</div>
  <div class="grid-item header">CSV2S (ours)</div>
  <div class="grid-item header">UniCUE (ours)</div>
</div>
<div class="grid-container">
  <div class="grid-item"><video controls src="videos/gt_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/wav2lip_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/sadtalker_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/makeittalk_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/audio2head_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/evag_01.mp4"></video></div>
  <div class="grid-item"><video controls src="videos/unicue_01.mp4"></video></div>
</div>

    <footer>
      <p>© 2025| Designed with ❤️ for open research</p>
    </footer>

  </div>
</body>
</html>
