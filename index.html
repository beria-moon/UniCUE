
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="generator" content="Hugo 0.88.1" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
	<link rel="stylesheet" href="css/custom.css">
	<link rel="stylesheet" href="css/normalize.css">
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
	<title>UinCUE</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
</head>
<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">
<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">
<div class="container pt-5 mt-5 shadow-lg p-5 mb-5 bg-white rounded text-center">
  <h2>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2>

  <p>
    <a href="">[Paper]</a>
    <span>&nbsp;</span>
    <a href="">[Dataset]</a>
    <span>&nbsp;</span>
    <a href="">[Code]</a>
  </p>

  <p class="fst-italic mb-0">
    Anonymous authors
  </p>
</div>

<h2 class="mt-5 mb-3 text-center">üìù Abstract</h2>
<div class="mx-auto" style="max-width: 900px; font-size: 1.1rem; line-height: 1.8;">
  <p>
    <strong>Cued Speech (CS)</strong> enhances lipreading through hand coding, helping the hearing-impaired better perceive spoken language.
    The <strong>CS Video-to-Speech (CSV2S)</strong> task aims to convert CS videos into intelligible speech. However, existing methods typically adopt a two-stage pipeline‚ÄîCued Speech Recognition (CSR) followed by text-to-speech synthesis‚Äîwhich suffers from error accumulation and modality misalignment.
  </p>
  <p>
    We propose <strong>UniCUE</strong>, the first unified framework for directly generating speech from CS videos. UniCUE integrates CSR to provide fine-grained visual-semantic cues, enabling more accurate and fluent speech synthesis.
  </p>
  <p>Key components of UniCUE include:</p>
  <ul>
    <li>A <strong>pose-aware visual processor</strong> that captures fine-grained lip-hand visual cues;</li>
    <li>A <strong>semantic alignment pool</strong> for precise visual-semantic mapping;</li>
    <li>A <strong>VisioPhonetic adapter</strong> for cross-task representation fusion.</li>
  </ul>
  <p>
    Extensive experiments on our newly collected <strong>Chinese Cued Speech dataset</strong> demonstrate the superiority of UniCUE over existing methods in both recognition and speech generation quality.
  </p>
</div>


<p><b>UniCUE Framework</b></p>
	<p style="text-align: center;">
		<img src="assets/pipeline.png" height="400" width="700">
</p>


    <h2>üìÅ Dataset</h2>
    <ul>
      <li>11,282 videos</li>
      <li>14 subjects (6 hearing, 8 deaf)</li>
      <li>Rich annotations: phonemes, open pose gestures,speech,video</li>
      <li>Format: MP4 +WAV+TXT</li>
    </ul>

    <h2>üí° Key Features</h2>
    <ul>
      <li>Diverse participants: inclusion of both deaf and hearing groups</li>
      <li>Promotes accessibility and cross-modal language research</li>
      <li>Useful for Cued Speech recognition and generation tasks</li>
    </ul>

    <footer>
      <p>¬© 2025| Designed with ‚ù§Ô∏è for open research</p>
    </footer>

  </div>
</body>
</html>
