
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="generator" content="Hugo 0.88.1" />
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700" rel="stylesheet" type="text/css">
	<link rel="stylesheet" href=""https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
	<link rel="stylesheet" href="css/custom.css">
	<link rel="stylesheet" href="css/normalize.css">

	<title>UinCUE</title>
	<link href="css/bootstrap.min.css" rel="stylesheet">
</head>
<body data-new-gr-c-s-check-loaded="14.1091.0" data-gr-ext-installed="">
<div class="container" >
<header role="banner">
</header>
<main role="main">
<article itemscope itemtype="https://schema.org/BlogPosting">
<div class="container pt-5 mt-5 shadow-lg p-5 mb-5 bg-white rounded text-center">
  <h2>UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation</h2>

  <p>
    <a href="">[Paper]</a>
    <span>&nbsp;</span>
    <a href="">[Dataset]</a>
    <span>&nbsp;</span>
    <a href="">[Code]</a>
  </p>

  <p class="fst-italic mb-0">
    Anonymous authors
  </p>
</div>

<h2>ğŸ“ Abstract</h2>
<p>
Cued Speech (CS) enhances lipreading through hand coding, helping the hearing-impaired better perceive spoken language. The CS Video-to-Speech (CSV2S) task aims to convert CS videos into intelligible speech. While existing methods rely on CS Recognition (CSR) followed by text-to-speech synthesis, this two-stage pipeline suffers from error accumulation and misalignment.

We propose UniCUE, the first unified framework for directly generating speech from CS videos. UniCUE integrates CSR to provide fine-grained visual-semantic cues, enabling more accurate and fluent speech synthesis. Key components include:

1.  A pose-aware visual processor capturing fine-grained lip-hand visual cues; 
2. A semantic alignment pool for precise visual-semantics mapping,
3. A VisioPhonetic adapter for cross-task representation fusion,
Extensive experiments on our newly collected Chinese CS dataset demonstrate UniCUEâ€™s superior performance over existing methods.
</p>

<p><b>UniCUE Framework</b></p>
	<p style="text-align: center;">
		<img src="assets/pipeline.png" height="400" width="700">
</p>


    <h2>ğŸ“ Dataset</h2>
    <ul>
      <li>11,282 videos</li>
      <li>14 subjects (6 hearing, 8 deaf)</li>
      <li>Rich annotations: phonemes, open pose gestures,speech,video</li>
      <li>Format: MP4 +WAV+TXT</li>
    </ul>

    <h2>ğŸ’¡ Key Features</h2>
    <ul>
      <li>Diverse participants: inclusion of both deaf and hearing groups</li>
      <li>Promotes accessibility and cross-modal language research</li>
      <li>Useful for Cued Speech recognition and generation tasks</li>
    </ul>

<!--     <h2>ğŸ“œ Citation</h2>
    <pre>
@article{wang2025unicue,
  title={UniCUE: A Large-scale Chinese Cued Speech Dataset from Deaf and Hearing Participants},
  author={Wang, Jinting and ...},
  journal={ArXiv preprint arXiv:xxxx.xxxxx},
  year={2025}
} -->
<!--     </pre> -->

<!--     <h2>ğŸ”— Related Links</h2>
    <ul>
      <li><a href="https://cuedspeech.org">CuedSpeech.org</a></li>
      <li><a href="https://github.com">More on GitHub</a></li>
    </ul> -->

    <footer>
      <p>Â© 2025| Designed with â¤ï¸ for open research</p>
    </footer>

  </div>
</body>
</html>
